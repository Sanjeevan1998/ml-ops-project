version: "3.8"

name: LexisRaw

volumes:
  lexisraw:

services:
  # Data Extraction (Download and Unzip from NYU Box)
  extract-data:
    container_name: etl_extract_data
    image: python:3.11
    user: root
    volumes:
      - lexisraw:/data
      - /home/cc/ml-ops-project/data-pipeline:/data-pipeline
    working_dir: /data
    command:
      - bash
      - -c
      - |
        set -e
        echo "üóÇÔ∏è Resetting dataset directory..."
        rm -rf raw_pdfs
        mkdir -p raw_pdfs
        cd raw_pdfs

        echo "üåê Downloading dataset from NYU Box..."
        curl -L https://nyu.box.com/shared/static/aiirowrixydy10weqxg3mmzvxdweumyy.zip -o LexisRaw.zip --verbose

        echo "üì¶ Unzipping dataset..."
        unzip -q LexisRaw.zip
        rm -f LexisRaw.zip

        echo "‚úÖ Data extracted to /data/raw_pdfs/"
        ls -lh /data/raw_pdfs/

  # Process Cases and Generate Metadata (Combined)
  processcases-and-createmetadata:
    container_name: etl_process_cases
    image: python:3.11
    user: root
    volumes:
      - lexisraw:/data
      - /home/cc/ml-ops-project/data-pipeline:/data-pipeline
    working_dir: /data
    command:
      - bash
      - -c
      - |
        set -e
        echo "üìù Extracting metadata and processing cases..."
        pip install -r /data-pipeline/requirements.txt
        python3 /data-pipeline/scripts/process_pdfs.py
        python3 /data-pipeline/scripts/create_metadata.py
        echo "‚úÖ Processed cases and metadata saved."

  # Embedding Generation
  create-embeddings:
    container_name: etl_create_embeddings
    image: python:3.11
    user: root
    volumes:
      - lexisraw:/data
      - /home/cc/ml-ops-project/data-pipeline:/data-pipeline
    working_dir: /data
    command:
      - bash
      - -c
      - |
        set -e
        echo "üîÑ Generating embeddings..."
        pip install -r /data-pipeline/requirements.txt
        python3 /data-pipeline/scripts/create_embeddings.py
        echo "‚úÖ Embeddings generated."

  # Triplet Generation
  generate-triplets:
    container_name: etl_generate_triplets
    image: python:3.11
    user: root
    volumes:
      - lexisraw:/data
      - /home/cc/ml-ops-project/data-pipeline:/data-pipeline
    working_dir: /data
    command:
      - bash
      - -c
      - |
        set -e
        echo "üîÑ Generating triplets..."
        pip install -r /data-pipeline/requirements.txt
        python3 /data-pipeline/scripts/generate_triplets.py
        echo "‚úÖ Triplets generated."

  # Data Upload
  load-data:
    container_name: etl_load_data
    image: rclone/rclone:latest
    volumes:
      - lexisraw:/data
      - ~/.config/rclone/rclone.conf:/root/.config/rclone/rclone.conf:ro
    environment:
      RCLONE_CONTAINER: object-store-persist-group36
    entrypoint: /bin/sh
    command:
      - -c
      - |
        if [ -z "$RCLONE_CONTAINER" ]; then
          echo "ERROR: RCLONE_CONTAINER is not set"
          exit 1
        fi

        echo "üöÄ Uploading data to container $RCLONE_CONTAINER"
      
        # Upload raw PDFs
        rclone copy /data/raw_pdfs chi_uc:$RCLONE_CONTAINER/raw_pdfs \
        --progress \
        --transfers=8 \
        --checkers=4 \
        --multi-thread-streams=2 \
        --fast-list

        # Upload processed cases
        rclone copy /data/processed chi_uc:$RCLONE_CONTAINER/processed \
        --progress \
        --transfers=8 \
        --checkers=4 \
        --multi-thread-streams=2 \
        --fast-list

        # Upload embeddings
        rclone copy /data/embeddings chi_uc:$RCLONE_CONTAINER/embeddings \
        --progress \
        --transfers=8 \
        --checkers=4 \
        --multi-thread-streams=2 \
        --fast-list

        echo "‚úÖ Data uploaded to object store."
        echo "Listing directories in container after load stage:"
        rclone lsd chi_uc:$RCLONE_CONTAINER

