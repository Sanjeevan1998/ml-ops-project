
services:
  legal-search-api:
    build: . # This will use your existing Dockerfile
    container_name: legal-search-api-dummy
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      # These local paths in the first part of the volume mapping (e.g., ./index)
      # are relative to your docker-compose.yaml file on the VM.
      # The second part (e.g., /app/index) is the path inside the container.
      # Ensure these align with what your main.py expects or what's set by ENV vars.
      - ./index:/app/index:ro
      - ./metadata:/app/metadata:ro
      - ./real_pdfs:/app/pdf_data:ro # This might be overridden by PDF_DATA_DIR if it points to object storage
      - ./hf_cache:/root/.cache/huggingface # For Hugging Face model caching
      - ./feedback_data:/app/feedback_data # For logging feedback
      # Mounts for pre-generated artifacts and models from persistent storage on the VM
      - /mnt/object-store-persist-group36:/app/mounted_bucket_storage:ro
      - /tmp/optimized_models:/app/optimized_models_local:ro # For ONNX models if exported to /tmp on VM

    environment:
      # --- Key setting for GPU deployment ---
      MODEL_DEVICE_PREFERENCE: "cuda" # Tells main.py to try and use CUDA

      # --- Ensure these match your desired runtime configuration ---
      # These will override any ENV set in the Dockerfile if uncommented.
      # If your Dockerfile ENV defaults are correct, you might not need all of them here.
      MODEL_TYPE_TO_LOAD: "ONNX" # Or "PYTORCH"
      ONNX_MODEL_PATH: "/app/optimized_models_local/legal_bert_finetuned_onnx_int8_quantized" # Path inside container
      EMBEDDING_MODEL: "/app/mounted_bucket_storage/model/Legal-BERT-finetuned" # Path inside container
      FAISS_INDEX_PATH: "/app/mounted_bucket_storage/faissIndex/v1/real_index.faiss" # Path inside container
      FAISS_MAP_PATH: "/app/mounted_bucket_storage/faissIndex/v1/real_map.pkl" # Path inside container
      METADATA_PATH: "/app/mounted_bucket_storage/faissIndex/v1/real_metadata.pkl" # Path inside container
      PDF_DATA_DIR: "/app/mounted_bucket_storage/LexisRaw" # Path inside container for PDF downloads

    # --- This 'deploy' section is CRUCIAL for GPU access ---
    # It tells Docker to make NVIDIA GPU(s) available to this container.
    # This will only work if the host VM has NVIDIA drivers & NVIDIA Container Toolkit.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 # Request 1 GPU. Can be "all" to request all available GPUs.
              capabilities: [gpu]
    networks:
      - monitoring_net

  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yaml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - monitoring_net

  grafana:
    image: grafana/grafana-oss:10.2.2
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: 'false'
    networks:
      - monitoring_net
    depends_on:
      - prometheus

networks:
  monitoring_net:
    driver: bridge

volumes:
  prometheus_data:
  grafana_data:
  hf_cache: