version: '3.8'

services:
  ray_minio_for_cluster: # MinIO service specifically for Ray's internal use
    image: minio/minio:RELEASE.2023-09-07T02-05-02Z
    container_name: ray_minio_for_cluster
    restart: unless-stopped
    ports:
      - "9310:9000"  # Host port remapped to avoid conflict with MLflow's MinIO
      - "9311:9001"  # Host port remapped
    environment:
      MINIO_ROOT_USER: "ray_lab_user_distinct"
      MINIO_ROOT_PASSWORD: "ray_lab_password_distinct"
    healthcheck:
      test: timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1
      interval: 5s
      timeout: 10s
      retries: 5
    command: server /data --console-address ":9001"
    volumes:
      - ray_lab_minio_data_vol:/data

  ray_minio_create_bucket_for_cluster: # Helper service to create a default bucket in Ray's MinIO
    image: minio/mc
    container_name: ray_minio_create_bucket_for_cluster
    depends_on:
      ray_minio_for_cluster:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      set -e;
      sleep 5;
      mc config host add minio_for_ray_lab_svc http://ray_minio_for_cluster:9000 ray_lab_user_distinct ray_lab_password_distinct;
      if ! mc ls minio_for_ray_lab_svc/ray; then
        mc mb minio_for_ray_lab_svc/ray &&
        echo 'Bucket ray created in ray_minio_for_cluster';
      else
        echo 'Bucket ray already exists in ray_minio_for_cluster';
      fi"

  ray-head:
    image: legalai-ray-env:latest # YOUR custom comprehensive image
    container_name: ray_head_lab_adapted
    user: root # Starts as root to set up permissions
    entrypoint: /bin/sh
    command:
      - -c
      - |
        set -e
        echo "Ray Head: Running as $(whoami) to prepare /tmp/ray"
        mkdir -p /tmp/ray
        chown -R jovyan:users /tmp/ray # Lab pattern for jovyan user
        echo "Ray Head: Permissions set for /tmp/ray. Executing Ray start as jovyan..."
        exec /usr/local/bin/gosu jovyan ray start \
          --head \
          --port=6379 \
          --dashboard-host=0.0.0.0 \
          --block \
          --metrics-export-port=8080 # For Prometheus scraping by Grafana
    ports:
      - "6379:6379"   # Ray GCS
      - "8265:8265"   # Ray Dashboard
      - "8080:8080"   # Ray Metrics export
    shm_size: '16gb'
    volumes:
      - ray_lab_tmp_vol:/tmp/ray # Ray internal temp files
      - /home/cc/ml-ops-project:/home/jovyan/work # Mount your project directory from host
    environment: # For Ray's S3 (internal MinIO) and Grafana integration
      - AWS_ACCESS_KEY_ID=ray_lab_user_distinct
      - AWS_SECRET_ACCESS_KEY=ray_lab_password_distinct
      - AWS_S3_ENDPOINT=http://ray_minio_for_cluster:9000
      - RAY_GRAFANA_HOST=http://grafana:3000 # Service name for Grafana
      - RAY_GRAFANA_IFRAME_HOST=http://${HOST_IP:-127.0.0.1}:3310
      - RAY_PROMETHEUS_HOST=http://ray_head_lab_adapted:8080 # Points to itself for metrics

  grafana: # Grafana for Ray metrics visualization
    image: grafana/grafana:latest
    container_name: ray_grafana_lab_adapted # Consistent naming with your file
    ports:
      - "3310:3000" # Host port remapped
    volumes:
      - ray_lab_grafana_storage_vol:/var/lib/grafana
      - ray_lab_tmp_vol:/tmp/ray:ro # Read-only access to Ray temp for some data sources
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_SECURITY_ALLOW_EMBEDDING=true
    depends_on:
      - ray-head

  ray-worker-1:
    image: legalai-ray-env:latest # YOUR custom comprehensive image
    container_name: ray_worker_1_lab_adapted
    user: root # Starts as root
    entrypoint: /bin/sh
    command:
      - -c
      - |
        set -e
        echo "Ray Worker: Running as $(whoami) to prepare /tmp/ray"
        mkdir -p /tmp/ray
        chown -R jovyan:users /tmp/ray # Lab pattern for jovyan user
        echo "Ray Worker: Permissions set for /tmp/ray. Executing Ray start as jovyan..."
        exec /usr/local/bin/gosu jovyan ray start --address=ray_head_lab_adapted:6379 --block
    shm_size: '16gb'
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock # If Ray needs to interact with Docker daemon
      - ray_lab_tmp_vol:/tmp/ray
      - /home/cc/ml-ops-project:/home/jovyan/work # Mount your project directory from host
    depends_on:
      - ray-head
    deploy: # GPU configuration for the worker
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
              count: 1 # Uses one GPU; adjust if multiple are on RTX_6000 and desired
    environment: # For Ray's S3 (internal MinIO) and NVIDIA driver visibility
      - AWS_ACCESS_KEY_ID=ray_lab_user_distinct
      - AWS_SECRET_ACCESS_KEY=ray_lab_password_distinct
      - AWS_S3_ENDPOINT=http://ray_minio_for_cluster:9000
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

volumes: # Named volumes for data persistence
  ray_lab_grafana_storage_vol:
  ray_lab_tmp_vol:
  ray_lab_minio_data_vol: