{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d2b402",
   "metadata": {},
   "source": [
    "## 3. Run Model Training on Ray Cluster\n",
    "\n",
    "Running our `legal_bert_triplet_finetune_a100.py` training script as a job on the Ray cluster. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919955b9",
   "metadata": {},
   "source": [
    "### Step 3.1: Navigate to Docker Training Directory\n",
    "\n",
    "The following command changes the current directory to `~/ml-ops-project/code/model-training/docker_training/`. This directory contains `docker-compose-ray-cuda.yaml` file needed to start the Ray cluster and potentially other Docker-related files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052255d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in CHI@UC GPU node terminal\n",
    "cd ~/ml-ops-project/code/model-training/docker_training/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52895b7",
   "metadata": {},
   "source": [
    "### Step 3.2: Ensure Clean Docker Environment\n",
    "\n",
    "Before starting the Ray cluster, it's good practice to stop any potentially conflicting Docker services. \n",
    "1. If there is a local MLflow stack running(from `docker-compose-mlflow.yaml`) on this node for testing, stop it now, as well' be using the centralized KVM@TACC MLflow.\n",
    "2. Stop any other standalone Docker containers (e.g., an `legalai_env` container) that might use the GPU or conflict with ports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93cb0e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this in CHI@UC GPU node terminal, if applicable\n",
    "\n",
    "# If local MLflow stack (from a different compose file) was running:\n",
    "# sudo docker compose -f docker-compose-mlflow.yaml down\n",
    "\n",
    "# If there's any specific standalone container like 'legalai_env':\n",
    "# sudo docker stop legalai_env\n",
    "# sudo docker rm legalai_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f379e60",
   "metadata": {},
   "source": [
    "### Step 3.3: Start the Ray Cluster\n",
    "\n",
    "First, ensure any previous instances of this Ray cluster are completely removed, including their volumes, to prevent state conflicts. Then, start the Ray cluster services (Ray head, Ray worker, internal MinIO, Grafana) in detached mode (`-d`) using your `docker-compose-ray-cuda.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb03b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in CHI@UC GPU node terminal (ensure the directory ~/ml-ops-project/code/model-training/docker_training/)\n",
    "\n",
    "# Clean up previous Ray cluster attempt (removes containers, networks, AND volumes)\n",
    "sudo docker compose -f docker-compose-ray-cuda.yaml down -v\n",
    "\n",
    "# Start the Ray cluster\n",
    "sudo docker compose -f docker-compose-ray-cuda.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5704faf1",
   "metadata": {},
   "source": [
    "### Step 3.4: Verify Ray Cluster Status\n",
    "\n",
    "Wait about 60 seconds for all services and healthchecks to initialize. Then, check if the Ray cluster components are running correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c9eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in your CHI@UC GPU node terminal\n",
    "sudo docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8612c3",
   "metadata": {},
   "source": [
    "There should be `ray_head_legalai` (ideally with `(healthy)` in its status if the healthcheck is passing), `ray_worker_1_legalai`, `ray_minio_for_internal_use`, and `ray_grafana_legalai` listed as `Up`.\n",
    "\n",
    "To check containers that might have exited (e.g., `ray_minio_internal_create_bucket` should exit with code 0 after success):\n",
    "`sudo docker ps -a`\n",
    "\n",
    "If `ray_head_legalai` or `ray_worker_1_legalai` are not `Up` or are restarting, check their logs:\n",
    "`sudo docker compose -f docker-compose-ray-cuda.yaml logs ray-head`\n",
    "`sudo docker compose -f docker-compose-ray-cuda.yaml logs ray-worker-1`\n",
    "\n",
    "try accessing the Ray Dashboard in your web browser: (e.g., `http://192.5.87.28:8265`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e70df",
   "metadata": {},
   "source": [
    "### Step 3.5: Set Environment Variables for Ray Job Submission\n",
    "\n",
    "These environment variables need to be set in the terminal session on CHI@UC GPU node from which we submit the Ray job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these commands in CHI@UC GPU node terminal\n",
    "# Ensure you are in a directory where app-cred-legalai-model-access-openrc.sh is accessible or provide its full path.\n",
    "# cd ~/ml-ops-project # Or similar\n",
    "\n",
    "echo \"Setting up KVM@TACC MLflow and MinIO environment variables...\"\n",
    "# Replace <KVM_MLFLOW_PORT>, <KVM_MINIO_API_PORT>, and KVM MinIO credentials with actual values.\n",
    "export MLFLOW_TRACKING_URI=\"http://129.114.27.166:5000\" \n",
    "export MLFLOW_S3_ENDPOINT_URL=\"http://129.114.27.166:9000\"\n",
    "export AWS_ACCESS_KEY_ID=\"YOUR_KVM_MINIO_ACCESS_KEY\"\n",
    "export AWS_SECRET_ACCESS_KEY=\"YOUR_KVM_MINIO_SECRET_KEY\"\n",
    "\n",
    "echo \"Sourcing CHI@UC Swift credentials...\"\n",
    "source ~/ml-ops-project/app-cred-legalai-model-access-openrc.sh\n",
    "\n",
    "echo \"Environment variables set. Verify OS_AUTH_URL is populated: $OS_AUTH_URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca49f9fe",
   "metadata": {},
   "source": [
    "### Step 3.6: Submit the Training Job to Ray Cluster\n",
    "\n",
    "Now, submit the Python training script (`legal_bert_triplet_finetune_a100.py`) to the running Ray cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4bd75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray job submit --address http://127.0.0.1:8265 \\\n",
    " --working-dir /home/jovyan/work/code/model-training/training_script/ \\\n",
    " --runtime-env-json '{\n",
    "    \"pip\": \"requirements.txt\",\n",
    "    \"env_vars\": {\n",
    "        \"MLFLOW_TRACKING_URI\": \"'\"${MLFLOW_TRACKING_URI}\"'\",\n",
    "        \"MLFLOW_S3_ENDPOINT_URL\": \"'\"${MLFLOW_S3_ENDPOINT_URL}\"'\",\n",
    "        \"AWS_ACCESS_KEY_ID\": \"'\"${AWS_ACCESS_KEY_ID}\"'\",\n",
    "        \"AWS_SECRET_ACCESS_KEY\": \"'\"${AWS_SECRET_ACCESS_KEY}\"'\",\n",
    "        \"OS_AUTH_URL\": \"'\"${OS_AUTH_URL}\"'\",\n",
    "        \"OS_IDENTITY_API_VERSION\": \"'\"${OS_IDENTITY_API_VERSION}\"'\",\n",
    "        \"OS_PROJECT_ID\": \"'\"${OS_PROJECT_ID}\"'\",\n",
    "        \"OS_PROJECT_NAME\": \"'\"${OS_PROJECT_NAME}\"'\",\n",
    "        \"OS_USER_DOMAIN_NAME\": \"'\"${OS_USER_DOMAIN_NAME}\"'\",\n",
    "        \"OS_PROJECT_DOMAIN_ID\": \"'\"${OS_PROJECT_DOMAIN_ID}\"'\",\n",
    "        \"OS_APPLICATION_CREDENTIAL_ID\": \"'\"${OS_APPLICATION_CREDENTIAL_ID}\"'\",\n",
    "        \"OS_APPLICATION_CREDENTIAL_NAME\": \"'\"${OS_APPLICATION_CREDENTIAL_NAME}\"'\",\n",
    "        \"OS_APPLICATION_CREDENTIAL_SECRET\": \"'\"${OS_APPLICATION_CREDENTIAL_SECRET}\"'\",\n",
    "        \"PYTORCH_CUDA_ALLOC_CONF\": \"expandable_segments:True\"\n",
    "    }\n",
    " }' \\\n",
    " -- python3 legal_bert_triplet_finetune_a100.py \\\n",
    "    --data_path \"/home/jovyan/work/code/model-training/training_data/legal_data.jsonl\" \\\n",
    "    --model_name_or_path \"swift://object-store-persist-group36/model/Legal-BERT/\" \\\n",
    "    --local_model_temp_dir \"/home/jovyan/work/temp_swift_downloads_ray_job\" \\\n",
    "    --output_dir \"/home/jovyan/work/sbert_output_ray_job\" \\\n",
    "    --num_epochs 1 \\\n",
    "    --batch_size 4 \\\n",
    "    --mlflow_experiment_name \"LegalAI-RayJob-KVM-MLflow\" \\\n",
    "    --mlflow_run_name \"rayjob-kvm-$(date +%Y%m%d-%H%M%S)-bs4-final\" \\\n",
    "    --dev_split_ratio 0.2 \\\n",
    "    --evaluation_steps 50 \\\n",
    "    --evaluate_base_model \\\n",
    "    --random_seed 42 \\\n",
    "    --upload_model_to_swift \\\n",
    "    --swift_container_name \"object-store-persist-group36\" \\\n",
    "    --swift_upload_prefix \"models/my_finetuned_legal_bert_rayjob_kvm/run_$(date +%Y%m%d-%H%M%S)_bs4-final\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec081cd",
   "metadata": {},
   "source": [
    "### Step 3.7: Monitor the Ray Job\n",
    "\n",
    "1.  **Terminal Output:** The `ray job submit` command will stream logs from the job to your terminal. Watch for progress and any error messages.\n",
    "2.  **Ray Dashboard:** Open `http://<CHI_UC_NODE_FLOATING_IP>:8265` (e.g., `http://192.5.87.28:8265`) in web browser. Navigate to the \"Jobs\" section to see the status, logs, and resource usage of your submitted job.\n",
    "3.  **MLflow UI (KVM@TACC):** Open `http://129.114.27.166:<KVM_MLFLOW_PORT>` in browser. Look for the experiment `LegalAI-RayJob-KVM-MLflow` and the specific run name used. The parameters, metrics, and artifacts (like the model's Swift URI if upload is successful) being logged here should be visible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b5225",
   "metadata": {},
   "source": [
    "### Step 3.8: Clean Up Ray Cluster\n",
    "\n",
    "Once training job is complete and the results are verified, stop and remove the Ray cluster and its associated Docker containers and volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd81add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in your CHI@UC GPU node terminal (in ~/ml-ops-project/code/model-training/docker_training/)\n",
    "sudo docker compose -f docker-compose-ray-cuda.yaml down -v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
