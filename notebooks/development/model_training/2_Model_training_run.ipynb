{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell changes the current directory on the server to `ml-ops-project/code/model-training/docker_training` where all the docker files are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ml-ops-project/code/model-training/docker_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command starts the MLflow services (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker compose -f docker-compose-mlflow.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command starts the Ray cluster services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker compose -f docker-compose-ray.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command runs a new Docker container in detached mode (`-d`) with GPU access (`--gpus all`). \n",
    "- It maps port 8888 of the host to port 8888 of the container (typically for Jupyter).\n",
    "- It mounts the host directory `/home/cc/ml-ops-project` to `/home/jovyan/work` inside the container, making your project files accessible.\n",
    "- It names the container `legalai-ray-env`.\n",
    "- It uses the image `jupyter-mlflow:latest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker run -d --gpus all \\\n",
    "          -p 8888:8888 \\\n",
    "          -v /home/cc/ml-ops-project:/home/jovyan/work \\\n",
    "          --name legalai-ray-env \\\n",
    "          jupyter-mlflow:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists all currently running Docker containers on the server. To check if the services started by Docker Compose (MLflow, Ray) and the manually run container (`legalai-ray-env`) are active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open an interactive bash shell session inside the running Docker container named `legalai-ray-env`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker exec -it legalai-ray-env /bin/bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following commands are intended to be run *inside* the Docker container that we just `exec`'d into.**\n",
    "\n",
    "This command sources (executes) the OpenStack RC file located at `/home/jovyan/work/app-cred-legalai-model-access-openrc.sh` (inside the container). This script sets up environment variables (like `OS_AUTH_URL`, `OS_APPLICATION_CREDENTIAL_ID`, etc.) necessary for our Python script to authenticate with OpenStack Swift (your object storage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source /home/jovyan/work/app-cred-legalai-model-access-openrc.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `MLFLOW_TRACKING_URI` environment variable *inside the container*. It points to our centralized MLflow server running on KVM@TACC at `http://129.114.27.166:8000`. The Python training script will use this URI to log experiments and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export MLFLOW_TRACKING_URI=\"http://129.114.27.166:8000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the current directory to run the training script (`legal_bert_triplet_finetune_a100.py`) *inside the container* to `/home/jovyan/work/code/training_script/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /home/jovyan/work/code/model-training/training_script/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main command to execute your Python training script (`legal_bert_triplet_finetune_a100.py`) *inside the container*. It passes several arguments to the script:\n",
    "- Paths for data, model input/output (these paths are relative to the container's filesystem, using the `/home/jovyan/work` mount point).\n",
    "- Training hyperparameters like number of epochs (1) and batch size(8).\n",
    "- MLflow configuration (tracking URI, experiment name, run name).\n",
    "- Flags for evaluation and uploading the fine-tuned model to Swift, along with Swift container details.\n",
    "\n",
    "**Note:** If you are running this after starting a Ray cluster with `docker compose -f docker-compose-ray.yaml up -d`, this direct execution of the script is *not* using the Ray cluster for distributed training or job management. To use Ray, you would typically use `ray job submit` from the host machine, pointing to this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 legal_bert_triplet_finetune_a100.py \\\n",
    "    --data_path \"/home/jovyan/work/code/model-training/training_data/legal_data.jsonl\" \\\n",
    "    --model_name_or_path \"swift://object-store-persist-group36/model/Legal-BERT/\" \\\n",
    "    --local_model_temp_dir \"/home/jovyan/work/temp_swift_downloads_docker\" \\\n",
    "    --output_dir \"/home/jovyan/work/sbert_output_swift_docker\" \\\n",
    "    --num_epochs 1 \\\n",
    "    --batch_size 8 \\\n",
    "    --mlflow_tracking_uri \"${MLFLOW_TRACKING_URI}\" \\\n",
    "    --mlflow_experiment_name \"LegalAI-Swift-Sklearn-In-Docker\" \\\n",
    "    --mlflow_run_name \"docker-run-swift-sklearn-$(date +%Y%m%d-%H%M%S)-v8-test\" \\\n",
    "    --dev_split_ratio 0.2 \\\n",
    "    --evaluation_steps 50 \\\n",
    "    --evaluate_base_model \\\n",
    "    --random_seed 42 \\\n",
    "    --upload_model_to_swift \\\n",
    "    --swift_container_name \"object-store-persist-group36\" \\\n",
    "    --swift_upload_prefix \"models/my_finetuned_legal_bert/run_$(date +%Y%m%d-%H%M%S)_v8\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
